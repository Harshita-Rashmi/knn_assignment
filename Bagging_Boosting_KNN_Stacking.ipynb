{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5160f58",
   "metadata": {},
   "source": [
    "Assignment Code: DA-AG-0010\n",
    "\n",
    "**Bagging & Boosting KNN & Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcc6fd",
   "metadata": {},
   "source": [
    "\n",
    "1. Question 1 : What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Goal Reduce variance and prevent overfitting.\n",
    "\n",
    "How it works\n",
    "1. Create multiple subsets of the training data using random sampling with replacement (bootstrapping).\n",
    "2. Train a separate model on each subset independently.\n",
    "3. Combine their predictions:\n",
    "   - For classification: majority vote\n",
    "   - For regression: average\n",
    "\n",
    "**Example:** Random Forest is a classic bagging method using decision trees.\n",
    "\n",
    "**Analogy:** Like asking 10 people the same question and taking the average of their answers.\n",
    "\n",
    " Boosting\n",
    "\n",
    "**Goal:** Reduce bias and improve accuracy by focusing on mistakes.\n",
    "\n",
    "**How it works:**\n",
    "1. Train the first model on the full dataset.\n",
    "2. Identify where it made mistakes.\n",
    "3. Train the next model to focus on those mistakes.\n",
    "4. Repeat this process, each model learning from the errors of the previous one.\n",
    "5. Combine all models using a **weighted sum** of their predictions.\n",
    "\n",
    "**Example:** AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "**Analogy:** Like a teacher correcting a student’s mistakes step by step until they master the topic.\n",
    "\n",
    " Summary Table\n",
    "\n",
    "| Feature            | **Bagging**                          | **Boosting**                                      |\n",
    "|--------------------|--------------------------------------|---------------------------------------------------|\n",
    "| **Goal**             Reduce variance                      | Reduce bias                                       |\n",
    "| **Training**       | Parallel (independent models)        | Sequential (each model learns from previous)      |\n",
    "| **Data Sampling**  | Bootstrapped subsets                 | Full data, reweighted based on errors             |\n",
    "| **Combining**      | Majority vote / average              | Weighted sum                                      |\n",
    "| **Example**        | Random Forest                        | AdaBoost, XGBoost                                 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d947ea",
   "metadata": {},
   "source": [
    "2. Explain how the Random Forest Classifier reduces overfitting compared to\n",
    "a single decision tree. Mention the role of two key hyperparameters in this process.\n",
    "\n",
    "\n",
    "\n",
    "A single decision tree tends to overfit the training data — it learns patterns too precisely, including noise, which hurts generalization on unseen data.\n",
    "Random Forest, on the other hand, builds multiple decision trees and combines their predictions. It reduces overfitting by introducing randomness and averaging:\n",
    "- Random sampling of data (bootstrapping): Each tree sees a different subset of the training data.\n",
    "- Random feature selection: Each tree considers only a random subset of features when splitting nodes.\n",
    "This diversity among trees ensures that no single tree dominates, and the ensemble is more robust and generalizes better.\n",
    "\n",
    "Two Key Hyperparameters That Help\n",
    "1. n_estimators (Number of Trees)\n",
    "- Controls how many decision trees are built.\n",
    "- More trees → better averaging → reduced variance.\n",
    "- But too many trees can increase training time without much gain.\n",
    "\n",
    "2. max_features (Number of Features Considered per Split)\n",
    "- Limits how many features each tree can use when making splits.\n",
    "- Lower values → more randomness → less correlation between trees → better generalization.\n",
    "- Common choices: \"sqrt\" for classification, \"log2\", or a fixed number.\n",
    "\n",
    " Intuition Recap\n",
    "- A single tree memorizes; a forest generalizes.\n",
    "- Random Forest trades a bit of bias for a big drop in variance — the sweet spot for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3dc0c",
   "metadata": {},
   "source": [
    "Question 3: What is Stacking in ensemble learning? How does it differ from traditional bagging/boosting methods? Provide a simple example use case.\n",
    "\n",
    "\n",
    "Stacking (Stacked Generalization) is an ensemble method that combines multiple different models (called base learners) and uses another model (called a meta learner) to learn how best to combine their predictions.\n",
    "How It Works:\n",
    "- Train several diverse base models (e.g., decision tree, SVM, logistic regression) on the same dataset.\n",
    "- Collect their predictions on a validation set.\n",
    "- Train a meta-model (often a simple model like logistic regression) on these predictions to make the final decision.\n",
    "The meta-model learns which base model to trust more for different parts of the data.\n",
    "\n",
    "\n",
    "Use Case: Loan Default Prediction\n",
    "Imagine you're building a model to predict whether a customer will default on a loan.\n",
    "- Base Models:\n",
    "- Decision Tree (captures non-linear patterns)\n",
    "- Logistic Regression (good for linear relationships)\n",
    "- K-Nearest Neighbors (captures local structure)\n",
    "- Meta Model:\n",
    "- Logistic Regression trained on the predictions of the three base models\n",
    "This stacked model can outperform any single model by leveraging the strengths of each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:What is the OOB Score in Random Forest, and why is it useful? How does it help in model evaluation without a separate validation set?\n",
    "\n",
    "\n",
    "What Is the OOB Score?\n",
    "OOB Score stands for Out-of-Bag Score. It’s a performance metric calculated using the data that wasn’t included in the bootstrap sample for each tree.\n",
    " How It Works:\n",
    "- Random Forest uses bootstrapping: each tree is trained on a random subset of the training data (with replacement).\n",
    "- On average, about one-third of the data is left out of each tree’s training set — these are called out-of-bag samples.\n",
    "- Each tree makes predictions on its own OOB samples.\n",
    "- The final OOB score is computed by aggregating predictions across all trees for their respective OOB samples and comparing them to the true labels.\n",
    "\n",
    " Why Is It Useful?\n",
    "- No need for a separate validation set: You can use all your data for training while still getting a reliable estimate of model performance.\n",
    "- Efficient: Saves time and resources, especially when data is limited.\n",
    "- Reliable: Often gives a good approximation of test accuracy, especially for - classification tasks.\n",
    "\n",
    "How It Helps in Model Evaluation\n",
    "- Acts like cross-validation, but is built into the Random Forest algorithm.\n",
    "- Helps detect overfitting: If your training accuracy is high but OOB score is low, your model might be memorizing the data.\n",
    "\n",
    "-code:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"OOB Score:\", rf.oob_score_)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 5.\n",
    "Compare AdaBoost and Gradient Boosting in terms of:\n",
    "● How they handle errors from weak learners\n",
    "● Weight adjustment mechanism\n",
    "● Typical use cases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AdaBoost vs Gradient Boosting \n",
    "\n",
    "AdaBoost handles errors by increasing the importance (weights) of misclassified samples after each weak learner, forcing the next learner to focus more on those difficult instances.\n",
    " In contrast, Gradient Boosting handles errors by fitting each new learner to the residuals (prediction errors) of the previous learners, effectively reducing the overall loss through gradient descent. \n",
    "\n",
    "In terms of weight adjustment, AdaBoost explicitly updates and redistributes sample weights after every iteration, while Gradient Boosting does not adjust sample weights directly; \n",
    "instead, it updates the model by adding a scaled prediction of the new learner to reduce the loss. \n",
    "\n",
    "AdaBoost is typically used for cleaner datasets and simpler classification tasks, often with decision stumps as weak learners, whereas Gradient Boosting is widely used in high-performance machine learning scenarios—including regression and complex classification problems—and is the foundation of advanced models like XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "\n",
    "AdaBoost:Effective for clean datasets, face detection, spam filtering, customer churn prediction, fraud detection when noise/outliers are limited.\n",
    "\n",
    "Gradient Boosting: Used for large-scale tabular data problems, credit scoring, sales forecasting, ranking systems, medical diagnosis models, and most competitive ML tasks (XGBoost/LightGBM/CatBoost).\n",
    "\n",
    "\n",
    "\n",
    "Use cases \n",
    "\n",
    "AdaBoost is commonly used when the dataset is relatively clean and not dominated by noise, making it suitable for tasks like face detection, spam classification, fraud identification, or customer churn prediction, especially when simpler weak learners are preferred.\n",
    "\n",
    "Gradient Boosting, on the other hand, is widely applied across high-performance machine learning applications and works well on complex, large-scale tabular datasets. \n",
    "\n",
    "It is frequently used in credit scoring, forecasting tasks, ranking and recommendation systems, and medical diagnosis models.\n",
    "\n",
    "Modern implementations such as XGBoost, LightGBM, and CatBoost make Gradient Boosting one of the most popular approaches for achieving top accuracy in machine learning competitions and real-world predictive analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 6. \n",
    "Why does CatBoost perform well on categorical features without requiring\n",
    "extensive preprocessing? Briefly explain its handling of categorical variables.\n",
    "\n",
    "\n",
    "CatBoost stands out because it natively handles categorical features without needing manual preprocessing like one-hot encoding or label encoding. Here's how it does that:\n",
    "\n",
    " Why CatBoost Excels with Categorical Features\n",
    "1. Ordered Target Statistics (a.k.a. Target Encoding with Permutations)\n",
    "- Instead of converting categories into dummy variables, CatBoost replaces each categorical value with a number based on the target variable.\n",
    "- It uses mean target encoding — for classification, this might be the probability of the target class given the category.\n",
    "- To avoid target leakage, it uses ordered boosting: it calculates the encoding using only previous data points, not the current one.\n",
    "Example: If you're predicting loan default and \"Job Type\" is a categorical feature, CatBoost might encode \"Engineer\" as the average default rate of engineers seen before the current row.\n",
    "\n",
    "\n",
    "2. Efficient Handling of High-Cardinality Features\n",
    "- CatBoost can handle features with many unique categories (like ZIP codes or product IDs) without exploding the feature space.\n",
    "- It uses combinations of categorical features and hashing to capture interactions without manual feature engineering.\n",
    "\n",
    "3. No Need for Manual Encoding\n",
    "- You can pass categorical columns directly as strings or specify their indices — CatBoost takes care of the rest.\n",
    "- This reduces preprocessing time and risk of introducing bias or overfitting through poor encoding choices.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e1279",
   "metadata": {},
   "source": [
    "7.KNN Classifier Assignment: Wine Dataset Analysis with\n",
    "Optimization\n",
    "\n",
    "Task:\n",
    "1. Load the Wine dataset (sklearn.datasets.load_wine()).\n",
    "2. Split data into 70% train and 30% test.\n",
    "3. Train a KNN classifier (default K=5) without scaling and evaluate using:\n",
    "a. Accuracy\n",
    "b. Precision, Recall, F1-Score (print classification report)\n",
    "4. Apply StandardScaler, retrain KNN, and compare metrics.\n",
    "5. Use GridSearchCV to find the best K (test K=1 to 20) and distance metric\n",
    "(Euclidean, Manhattan).\n",
    "\n",
    "Answer:-\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy (No Scaling):\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report (No Scaling):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy (With Scaling):\", accuracy_score(y_test, y_pred_scaled))\n",
    "print(\"Classification Report (With Scaling):\\n\", classification_report(y_test, y_pred_scaled))\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 21)),\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_knn.best_params_)\n",
    "print(\"Best Cross-Validated Score:\", grid_knn.best_score_)\n",
    "\n",
    "# Final evaluation on test set\n",
    "best_knn = grid_knn.best_estimator_\n",
    "y_final_pred = best_knn.predict(X_test_scaled)\n",
    "\n",
    "print(\"Final Accuracy (Optimized):\", accuracy_score(y_test, y_final_pred))\n",
    "print(\"Final Classification Report:\\n\", classification_report(y_test, y_final_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebf9f89",
   "metadata": {},
   "source": [

    "Question 8 :

PCA + KNN with Variance Analysis and Visualization\n",
    "Task:\n",
    "1. Load the Breast Cancer dataset (sklearn.datasets.load_breast_cancer()).\n",
    "2. Apply PCA and plot the scree plot (explained variance ratio).\n",
    "3. Retain 95% variance and transform the dataset.\n",
    "4. Train KNN on the original data and PCA-transformed data, then compare\n",
    "accuracy.\n",
    "5. Visualize the first two principal components using a scatter plot (color by class).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Scree Plot: Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca_95 = PCA(n_components=0.95)\n",
    "X_pca_95 = pca_95.fit_transform(X_scaled)\n",
    "print(f\"Number of components to retain 95% variance: {pca_95.n_components_}\")\n",
    "\n",
    "\n",
    "# Split original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN on original data\n",
    "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_orig.fit(X_train, y_train)\n",
    "y_pred_orig = knn_orig.predict(X_test)\n",
    "acc_orig = accuracy_score(y_test, y_pred_orig)\n",
    "\n",
    "# Split PCA-transformed data\n",
    "X_train_pca, X_test_pca, _, _ = train_test_split(X_pca_95, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN on PCA data\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"Accuracy on Original Data: {acc_orig:.4f}\")\n",
    "print(f\"Accuracy on PCA-Transformed Data: {acc_pca:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='Set1', alpha=0.7)\n",
    "plt.title('PCA: First Two Principal Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(title='Class')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034207af",
   "metadata": {},
   "source": [

    "Question 9:
KNN Regressor with Distance Metrics and K-Value\n",
    "Analysis\n",
    "Task:\n",
    "1. Generate a synthetic regression dataset\n",
    "(sklearn.datasets.make_regression(n_samples=500, n_features=10)).\n",
    "2. Train a KNN regressor with:\n",
    "a. Euclidean distance (K=5)\n",
    "b. Manhattan distance (K=5)\n",
    "c. Compare Mean Squared Error (MSE) for both.\n",
    "3. Test K=1, 5, 10, 20, 50 and plot K vs. MSE to analyze bias-variance tradeoff.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_regression(n_samples=500, n_features=10, noise=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Euclidean distance (default)\n",
    "knn_euclidean = KNeighborsRegressor(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "mse_euclidean = mean_squared_error(y_test, y_pred_euclidean)\n",
    "\n",
    "# Manhattan distance\n",
    "knn_manhattan = KNeighborsRegressor(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "mse_manhattan = mean_squared_error(y_test, y_pred_manhattan)\n",
    "\n",
    "print(f\"MSE (Euclidean, K=5): {mse_euclidean:.2f}\")\n",
    "print(f\"MSE (Manhattan, K=5): {mse_manhattan:.2f}\")\n",
    "\n",
    "\n",
    "k_values = [1, 5, 10, 20, 50]\n",
    "mse_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, mse_scores, marker='o', linestyle='-', color='teal')\n",
    "plt.title('K vs. Mean Squared Error')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b4418",
   "metadata": {},
   "source": [
    "Question 10:

KNN with KD-Tree/Ball Tree, Imputation, and Real-World\n",
    "Data\n",
    "Task:\n",
    "1. Load the Pima Indians Diabetes dataset (contains missing values).\n",
    "2. Use KNN Imputation (sklearn.impute.KNNImputer) to fill missing values.\n",
    "3. Train KNN using:\n",
    "a. Brute-force method\n",
    "b. KD-Tree\n",
    "c. Ball Tree\n",
    "4. Compare their training time and accuracy.\n",
    "5. Plot the decision boundary for the best-performing  method (use 2 most important\n",
    "features).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\n",
    "df = pd.read_csv(url, names=columns)\n",
    "\n",
    "# Replace zeroes with NaN for features that shouldn't be zero\n",
    "zero_features = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "df[zero_features] = df[zero_features].replace(0, pd.NA)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=columns)\n",
    "\n",
    "\n",
    "X = df_imputed.drop('Outcome', axis=1)\n",
    "y = df_imputed['Outcome']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define methods\n",
    "methods = ['brute', 'kd_tree', 'ball_tree']\n",
    "results = {}\n",
    "\n",
    "for algo in methods:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
    "    start = time.time()\n",
    "    knn.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[algo] = {'accuracy': acc, 'training_time': end - start}\n",
    "\n",
    "# Display results\n",
    "for method, metrics in results.items():\n",
    "    print(f\"{method.upper()} → Accuracy: {metrics['accuracy']:.4f}, Training Time: {metrics['training_time']:.4f} seconds\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Use top 2 features by correlation with target\n",
    "corr = df_imputed.corr()['Outcome'].abs().sort_values(ascending=False)\n",
    "top2 = corr.index[1:3]\n",
    "X2 = df_imputed[top2]\n",
    "X2_scaled = scaler.fit_transform(X2)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train best model (e.g., KD-Tree)\n",
    "best_knn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
    "best_knn.fit(X2_train, y2_train)\n",
    "\n",
    "# Decision boundary plot\n",
    "h = .02\n",
    "x_min, x_max = X2_scaled[:, 0].min() - 1, X2_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X2_scaled[:, 1].min() - 1, X2_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = best_knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "sns.scatterplot(x=X2_scaled[:, 0], y=X2_scaled[:, 1], hue=y, palette=cmap_bold, edgecolor='k')\n",
    "plt.title('Decision Boundary (KNN with KD-Tree)')\n",
    "plt.xlabel(top2[0])\n",
    "plt.ylabel(top2[1])\n",
    "plt.grid(True)\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
